General Machine Learning Concepts
    What is the difference between supervised and unsupervised learning?
    - Supervised learning starts with labelled data where each set of inputs is paired with an output
    - It aims to learn the patterns which enable it to predict the output
    - You can assess the accuracy of each prediction against the true correct answer
    - e.g. using a CNN to predict whether an image is a cat or a dog
    - With unsupervised learning you don't have labelled data
    - It aims to detect underlying patterns in the data
    - e.g grouping similar news articles

    Explain the bias-variance trade-off.
    - Bias relates to how well your model fits your training data with low bias indicating a 'good' fit
    - Variance relates to how sensitive your model is to the training data with low variance indicating it is
      not sensitive
    - High bias means the model is underfit, it fails to capture the complexity withing the training data
    - Low bias means the model fits the training data well however it could be overfit to the training data meaning it
      captures too much of the noise and doesn't generalise well to unseen data
    - It's referred to as a trade-off as sacrificing bias can often lead to better variance

    What are overfitting and underfitting? How can they be addressed?
    - Underfitting is when a model fails to capture the underlying complexity with the data (high bias)
    - Overfitting is when a model capture too much of the noise within the data and so doesn't generalise well to unseen
      data (low bias, high variance)
    - In general, underfitting is addressed by increasing the complexity of the model whereas overfitting is addressed
      by reducing the complexity of the model.
    - For example with underfitting you could
      - Increase the number of variables
      - Use feature engineering to create more complex variables
      - Increase the model complexity (e.g. more trees, greater depth, more node in a neural network)
    - For overfitting you could:
        - Decrease the number of variables in the model
        - Simply variable transformations to construst simpler features
        - Reduce the model complexity (e.g. fewer trees, less tree depth, fewer layers in a neural network
        - Apply regularisation to penalise large coefficients and ensure the neural network isn't overly dependent on a few variables
        - Implement cross-validation to ensure the model performs well on unseen data

    Describe the steps involved in a machine learning project, from data collection to model deployment.
    - Identify the problem you are trying to solve, the data you need to collect, the approach you will take and how you
      will measure the outcome
    - Collect the data
    - Perform EDA to understand the data
    - Process the data
    - Build models (test using cross-validation)
    - Test models
    - Design deployment
    - Design monitoring
    - Design deployment experiment
    - Deploy model and monitor
    - Iterate model, testing against current setup

    What are some common metrics used to evaluate classification models?
    - Accuracy: The number of correctly categorised predictions
    - Recall (Sensitivity): The % of positives correctly categorised
    - Precision: The % of times the model was correct when categorising as positive
    - F1-score: The harmonic mean of recall and precision
    - Specificity: The % negatives correctly categorised
    - Area Under the Receiver Operating Characteristic Curve (AUC-ROC): The ROC curve plots the true positive rate
        (recall) against the false positive rate (1-Specificity). The area under this curve measures model's ability to
        distinguish between the  classes

    Explain the difference between precision and recall.
    - Recall measures the proportion of actual positives the model correctly identified
    - Precision measures how often the model is correct when it categorises as positive
    - These are often used in imbalanced data sets where there is greater desire to correctly identify the positive
      category (e.g. brain cancer scans)

    What is cross-validation, and why is it important?
    - We want to be able to estimate how well a model will perform in production before we deploy it
    - However we cannot use the same data to test a model as it was trained with as this introduces bias
    - So instead we leave out a portion of the data from the training set and this is used to test the trained model
    - Typically this is implemented using k-fold cross-validation where the data is split into k groups and we train k
      separate models, each time leaving out 1 of the folds to use as unseen test data

    Can you explain the concept of a confusion matrix?
    - Used in classification problems
    - Plots the actual category against the predicted category with the numbers being counts

    What is regularization, and why is it used?
    - Regularisation aims to reduce the complexity of a model and thus prevent over fitting
    - It does this modifying the loss function in a such a way to penalise the model if it becomes overly complex
    - L1 regularisation adds a penalty proportional to the absolute value of the coefficients
    - L2 regularisation adds a penalty proportional to the square of the coefficients
    - Both L1 and L2 regularisation prevents the model from becoming overly dependent on a few variables
    - Dropout (for Neural Networks) works by randomly ignoring a fraction of the nodes during training, thus preventing
      the model from becoming overly reliant on certain paths

    How do you handle missing data?
    - First identify if there is a pattern to how the data is missing or if it as random
    - If it is at random, consider imputing:
        - For continuous variables use the mean if the data is normally distributed or median if the data is skewed
        - For discrete variables use the mode (or sample from a distribution)
        - Either impute from the entire data set or use k-nearest neighbours to find the data most similar to the sample
          with missing data and compute the impute value from just these samples
        - You could also try regression imputing where you try to predict the missing value using a linear model built
          on the other variables
        - For time series data, use linear interpolation to estimate missing values based  on the values before and
          after the missing data point

Algorithms and Techniques

    Describe the k-nearest neighbors (KNN) algorithm.
    - Plots data in higher dimensional vectorspace
    - Uses linear algebra (e.g. Euclidean distance) to find the points closest in vector space

    What is a decision tree, and how does it work?
    - Consists of nodes and branches and leaves
        - Nodes contain a statement about data (e.g. is male)
        - Branches correspond to True or false
        - Leaves are bins where the data end up
    - Data traverses the tree, answering the questions and ends up in a leaf
    - They can be used for classification or regression (with regression you usually take the mean of the values in the
      leaf as the prediction)
    - We decide which variable should be used in each node based on an evaluation metric (e.g. Gini impurity for
      classification or mean square error for regression)
    - When choosing the threshold to use for a continuous variable in a node, we order the values and test each
      pair-wise average, choosing the one with the best score on the evaluation metric
    - Gini Impurity = 1 âˆ’ sum_over_classes((% samples belonging to class i)^2)

    Explain the concept of ensemble learning and give examples of algorithms that use it.
    - Build many models to predict the outcome then aggregate the results
    - E.g. random forests are built from many decision trees
    - Could take the mode for classification or mean for regression
    - Generally perform better and are less overfit than single models

    What is the difference between bagging and boosting?
    - Bootstrapping is where you sample from your training dataset with replacement until you have a set the same
      overall size as the original training data
    - Bagging is where you create many bootstrapped samples and use each one to train a separate model then aggregate
      the predictions
    - Boosting is where you build models sequentially where each model corrects the mistakes of the previous model by
      trying to predict its residuals rather than the target

    What is gradient descent, and how does it work?
    - Gradient decent is the algorithm used to find the optimal value of a parameter
        1. Initialise a value for the parameter
        2. Use the model with that value to make predictions
        3. Take the derivative of the equation for the sum of squared residuals (SSR) with respect to the parameter
           (this gives the slope of the graph of SSR against the parameter)
        4. Fill in the values to get an actual value for the slope
        5. Multiple this slope by a learning rate to obtain the step size and subtract this from the current value for
           the parameter
        6. Repeat 2-5 until the step size is below a threshold

    Explain the working of a random forest.
    1. Create a bootstrapped dataset (i.e. sample from the original data set with replacement)
    2. Build a decision tree on the bootstrapped data set using a subset of the variables
        - You could also limit the number of leaves and/or layers
    3. Repeat 1-2 until a given number of trees have been build
    4. To make a prediction, run the data through every tree in the forest and aggregate the result
        - Use mode for classification
        - Use mean for regression

    What is the purpose of activation functions in neural networks?
    - They act as building blocks that can be stretched, flipped and scaled to fit complicated patterns to the data

    Explain the concept of feature engineering.
    - Feature engineering is the process of taking raw data and transforming it into an input to a machine learning
      model
    - It encompasses many possible processes such as:
        - Removing features which are irrelevant or cannot be used for prediction (e.g. ids)
        - Handling missing values (e.g. via imputation)
        - Transforming a feature using polynomial or logs to capture a more complex pattern
        - One-hot encoding categorical data so that ML models can interpret them
        - Standardization: Rescaling features to have a mean of 0 and a standard deviation of 1 - helps when dealing
          with variables with different units by putting them in terms of stand deviations
        - Normalisation: Rescaling features to a specific range, typically [0, 1] - this ensures variables contribute
          equally to the model regardless of their scale
        - Turning a raw image or text into a matrix of numbers

Practical Considerations and Techniques

    How would you handle imbalanced datasets?
    - Adjust evaluation metric (e.g. use F1-score instead of accuracy)
    - Oversample from the minority class
    - Augment data from the minority to create more variations (e.g. with images)
    - Apply class weights which penalise the model more for incorrectly classifying one class over another

    Describe a situation where you would use a logistic regression model.
    - When you are trying to predict a binary outcome such as is a drug effective or not

    What is dimensionality reduction, and why is it important?
    - The process of reducing the number of features in your dataset without loosing to much of the information those
      features contain
    - Helps to simplify the model making training faster

    Can you explain Principal Component Analysis (PCA)?
    - Used to reduce the dimensionality of a dataset while preserving as much variance (information) as possible
    - It transforms the data into a principal components which are  are ordered by the amount of variance they capture
      from the data
    1. Standardise the data to have a mean of 0 and a standard deviation of 1
    2. Compute the covariance matrix
    3. Compute the eigenvalues and eigenvectors then sort them by eigenvalues
    4. Choose a subset of the eigenvectors, these become the principal components
    5. Projcet the data using the eigenvectors - this is the reduced dimensionality data

    What are the key assumptions of linear regression?
    - Target is a linear combination of the variables
    - Residuals are independently and identically normally distributed and homoskedastic
    - Variables are not highly correlated with each other (no multicollinearity)

    How do you deal with categorical variables in your dataset?
    - One hot encoding

Advanced Topics

    What is transfer learning, and when is it useful?
    - Transfer learning is where you take a highly complex model which has been trained on a large and diverse data set
      and fine tune it to fit your specific problem
    - The model with have a large and rich feature set and you only train the final layer
    - Often works better than trying to build a model from scratch yourself

    Describe the architecture of a Convolutional Neural Network (CNN).

    What is a Recurrent Neural Network (RNN), and where is it used?

    What is the role of hyperparameter tuning in machine learning?
    -

Problem-Solving and Case Studies

    Describe a machine learning project you have worked on from start to finish.

    How would you explain a complex model to a non-technical audience?

    How would you improve an existing machine learning model?
    - More data and/or more recent data
    - New features
    - Hyperparamter tuning
    - Different model or architecture
