{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b17b66",
   "metadata": {},
   "source": [
    "# 04 - Hypothesis Testing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9191345",
   "metadata": {},
   "source": [
    "### **Hypothesis Tests**\n",
    "Hypothesis testing is a statistical method used to make decisions about a population based on sample data. The steps are as follows:\n",
    "    \n",
    "1. State the null hypothesis\n",
    "   - This is a statement about the assumed current distribution of the data and acts as a default to be disproved\n",
    "   \n",
    "2. State the alternative hypothesis\n",
    "   - This is a statement that there is an effect or a difference, it's what the test aims to support\n",
    "\n",
    "3. Choose a significance level, $\\alpha$\n",
    "   - $\\alpha$ is the probability of rejecting the null hypothesis when it is actually true. \n",
    "   - Commonly used values are 0.05 or 0.01\n",
    "\n",
    "4. Collect data and calculate the test statistic\n",
    "   - Data is collected from a sample, and a test statistic is calculated\n",
    "   - The test statistic summarises the data in a way that allows for the testing of hypotheses\n",
    "\n",
    "4. Determine the P-value\n",
    "   - The P-value is the probability of obtaining a test statistic at least as extreme as the one observed, assuming the null \n",
    "   hypothesis is true.\n",
    "   - Alternatively, a critical region can be determined, and the decision can be made based on whether the test statistic falls in the \n",
    "   critical region.\n",
    "\n",
    "5. Reject or fail to reject the alternative hypothesis\n",
    "   - If the P-value is less than the chosen significance level $\\alpha$, reject the null hypothesis in favor of the alternative hypothesis\n",
    "   - Otherwise, fail to reject the null hypothesis.\n",
    "\n",
    "Hypothesis testing helps determine whether observed data deviates significantly from what is expected under the null hypothesis. In the event we reject the null hypothesis, hypothesis testing does not tell us what the new value of the parameter being tested should be, merely that is varies significantly from the null hypothesis.\n",
    "\n",
    "The example below deomstrates a hypothesis test in which we test whether a coin flip is fair (P(heads) = P(tails) = 0.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db0e6570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of flips: 100\n",
      "Number of heads: 80\n",
      "p-value: 1.1159089057251951e-09\n",
      "Reject the null hypothesis: The coin is not fair.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Define the null hypothesis proportion (fair coin)\n",
    "p_null = 0.5\n",
    "\n",
    "# Define the observed data\n",
    "number_of_flips = 100\n",
    "number_of_heads = 80  # Try different values\n",
    "\n",
    "# Perform the binomial test by computing the p value which represents the chance, assuming the null hypothesis is true, that we get data\n",
    "# at least as extreme as the observed data (i.e. more than number_of_heads heads)\n",
    "\n",
    "# The alternative parameter specifies the alternative hypothesis:\n",
    "# 'two-sided' means the alternative hypothesis is that the coin is not fair (p != 0.5)\n",
    "binom_test = stats.binomtest(number_of_heads, number_of_flips, p_null, alternative='two-sided')\n",
    "\n",
    "# Output the result\n",
    "print(f\"Number of flips: {number_of_flips}\")\n",
    "print(f\"Number of heads: {number_of_heads}\")\n",
    "print(f\"p-value: {binom_test.pvalue}\")\n",
    "\n",
    "# Determine the conclusion\n",
    "alpha = 0.05  # significance level\n",
    "if binom_test.pvalue < alpha:\n",
    "    print(\"Reject the null hypothesis: The coin is not fair.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The coin is fair.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4dd07e",
   "metadata": {},
   "source": [
    "### **Bootstrapping in Hypothesis Tests**\n",
    "Suppose you have two datasets and want to assess whether their means are statistically different, but you're unsure about the underlying distributions â€” perhaps they appear non-normal or skewed. Traditional parametric tests like the t-test rely on assumptions about normality or equal variance, which may not hold true here. \n",
    "\n",
    "Bootstrapping is a non-parametric resampling technique that can help in this situation. It allows us to approximate the sampling distribution of a statistic (e.g., the mean) without assuming a specific form for the population distribution. We resample from our original dataset with replacement, compute sample means use these sample means to build a confidence interval which tells us whether the means or the original distributions are statistically different. \n",
    "\n",
    "In practice, it works as follows:\n",
    "1. Sample from the two datasets with replacement to get n samples of size m for each dataset. Usually m is the size of the entire    dataset.  \n",
    "2. Compute the means of each of the n samples\n",
    "3. Subtract the means of the samples from one dataset from the means of the samples from the other\n",
    "4. Empiraclly compute the 2.5th and 97.5th percentiles of the bootstrap distribution of mean differences to obtain a 95% confidence interval.\n",
    "5. If this interval does not contain 0 then the means are statistically different at a 5% significance level. \n",
    "\n",
    "The example below compares the means accross two non-normal distributions using bootstrapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f711fbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval: [-7.16947928 -6.95331734]\n",
      "The mean of dataset A is statistically different from the mean of dataset B at a 5% significance level\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 0: Generate Sample Data (can be non-normal)\n",
    "A = np.random.beta(0.5, 0.5, 1000)\n",
    "B = np.random.uniform(5, 10, 700)\n",
    "\n",
    "# 1: Sample with replacement to get n bootstrap samples\n",
    "n_samples = 10000\n",
    "A_bootstrap = np.random.choice(A, (n_samples, 1000), replace=True)\n",
    "B_bootstrap = np.random.choice(B, (n_samples, 700), replace=True)\n",
    "\n",
    "# 2: Calculate the means of each bootstrap sample\n",
    "A_sample_means = A_bootstrap.mean(axis=1)\n",
    "B_sample_means = B_bootstrap.mean(axis=1)\n",
    "\n",
    "# 3: Compute differences between sample means\n",
    "differences = A_sample_means - B_sample_means\n",
    "\n",
    "# 4: Empiraclly compute 95% confidence interval\n",
    "confidence_interval_95 = np.percentile(differences, [2.5, 97.5])\n",
    "print(f\"95% Confidence Interval: {confidence_interval_95}\")\n",
    "\n",
    "if max(confidence_interval_95) < 0 or min(confidence_interval_95) > 0:\n",
    "    print(\"The mean of dataset A is statistically different from the mean of dataset B at a 5% significance level\")\n",
    "else:\n",
    "    print(\"The mean of dataset A is not statistically different from the mean of dataset B at a 5% significance level\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
