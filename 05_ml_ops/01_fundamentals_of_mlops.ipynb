{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ec39f9",
   "metadata": {},
   "source": [
    "# 01 - Fundamentals of MLOps\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe594c63",
   "metadata": {},
   "source": [
    "### **Introduction**\n",
    "In a similar way to how DevOps is the process of managing and deploying code, MLOps is the process of managing and deploying machine learning models. However, MLOps presents a different set of challenges. With DevOps, changes to the behaviour of the deployed code are predominantly driven by changes to the code. A developer introduces a bug by mistake and this leads to a problem with how the deployed code functions. However changes to a machine learning systems could be driven by:\n",
    "- Data or distribution changes\n",
    "- Labels arriving late\n",
    "- Stochastic experiments\n",
    "- Divergence of training and inference environments\n",
    "\n",
    "MLOps exists to:\n",
    "- Make experiments reproducible\n",
    "- Track what produced a model\n",
    "- Make deployments safe\n",
    "- Detect degradation\n",
    "- Recover safely"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd858a14",
   "metadata": {},
   "source": [
    "### **The ML Lifecycle**\n",
    "The ML lifecycle consists of the following phases:\n",
    "1. **Problem Framing**\n",
    "- Clearly, define the objective\n",
    "- Translate into a machine learning task\n",
    "- Define success metrics\n",
    "- Identify constraints\n",
    "2. **Data**\n",
    "- Collect data and perform quality checks\n",
    "- Conduct feature engineering\n",
    "- Split into train, validation and test sets\n",
    "3. **Model Development**\n",
    "- Train models in a reproducible way\n",
    "- Conduct hyperparameter tuning\n",
    "- Track experiments (metrics, parameters, artifacts etc.)\n",
    "4. **Model Validation**\n",
    "- Cross-validation\n",
    "- Bias & fairness checks\n",
    "- Check performance agaisnt business metrics\n",
    "5. **Packaging & Deployment**\n",
    "- Containerise model and deploy (either batch or real-time serving)\n",
    "- CI/CD for ML\n",
    "6. **Monitoring in Production**\n",
    "- Monitor predictions\n",
    "- Monitor latency if required\n",
    "- Monitor input data and detect data drift\n",
    "- Detect concept drift\n",
    "7. **Retraining & Continuous Improvement**\n",
    "- Trigger-based retraining (e.g. time, drift, performance drop)\n",
    "- Automated retraining pipelines\n",
    "- Track model versions with rollback mechanisms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5527071",
   "metadata": {},
   "source": [
    "### **Reproducibility**\n",
    "**Reproducibility** in ML is the ability to reliably recreate a model exactly as it was originally trained. This is important because:\n",
    "- It enables debugging of production issues\n",
    "- It allows safe rollback to a previous model version\n",
    "- It ensures fair comparison between experiments\n",
    "- It enables collaboration across teams\n",
    "- It supports audit and regulatory requirements\n",
    "\n",
    "In practice, reproducibility means you must be able to recreate:\n",
    "- The data             \n",
    "- The code\n",
    "- The feature pipeline\n",
    "- The hyperparameters\n",
    "- The random seed\n",
    "- The execution environment\n",
    "\n",
    "Modern ML platforms such as Azure ML, AWS SageMaker, and Google Vertex AI provide built-in mechanisms to help manage these components in a structured and reproducible way. They generally share many of the same fundamental ideas about what should be tracked, albeit their implementations differ. The core ideas about how reproducibility is achieved are discussed below. \n",
    "\n",
    "**Data Reproducibility**\n",
    "Data is made reproducible by using **data versioning** which is where specific snapshots of the data are saved as named versions. These named versions are used as inputs for training jobs. Since the data is immutable, the exact dataset used to train a model can always be retrieved and reused.\n",
    "\n",
    "**Code Reproducibility**\n",
    "Code is made reproducible my using version management tools such as Git. By recording the repository, branch and commit ID, the exact training logic to be reconstructed at any point in time.\n",
    "\n",
    "**Feature Pipeline Reproducibility**\n",
    "Feature engineering is the process of turning your raw data in processed data ready for training and is often one of the largest sources of inconsistency. To ensure reproducibility:\n",
    "- Feature transformations must be defined in code (not performed manually)\n",
    "- Preprocessing steps must be part of the training pipeline\n",
    "- The exact feature set used for training must be recorded (i.e. via data versioning)\n",
    "- Training and inference pipelines must be identical\n",
    "\n",
    "**Hyperparameter Reproducibility**\n",
    "The hyperparameters can alter the performance of a model significantly thus recording which values were used as part of a training process is extremely important for reproducibility. You should record all hyperparameter values including search strategies (e.g. grid search, random search, Bayesian optimisation) and the search space. Most experiment tracking tools typically log these by default.\n",
    "\n",
    "**Randomness Control**\n",
    "Many ML incorporate some kind of randomness. For example, with gradient decent, generally the parameters are initialised with random values. A random seed is an ID value used by a random number generator, ensuring that the sequence of “random” numbers it produces is the same each time. Setting and recording the random seed allows for the stochastic elements of ML algorithms random elements to be controlled for, thus enabling the training process to be recreated exactly, despite the randomness.  \n",
    "\n",
    "**Environment Reproducibility**\n",
    "The environment an ML training pipeline is run in consists of the following:\n",
    "- The programming language version (e.g. Python 3.10)\n",
    "- The packages and their versions (e.g pandas 2.2.2)      \n",
    "- The operating system and its version (e.g. linux 24.04.1)            \n",
    "- Hardware drivers and their versions (e.g. NVIDIA CUDA 12.2)\n",
    "\n",
    "Each of these elements influences the output of a model training pipeline and so need to be controlled for. Reproducibility is most effectively achieved through Docker containerisation which ensures the elements listed above are packaged together in a consistent and portable way. Note that containerisation is discussed separately in futher detail ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523bb713",
   "metadata": {},
   "source": [
    "### **Experiment Tracking**\n",
    "During the model development phase, usually many models are trained using different data versions, feature sets, algorithms and hyperparameters. This neccessitates a structured method for recording the training process including the inputs, settings and outputs. **Experiment tracking** provides a systematic way to log and compare different training runs. \n",
    "\n",
    "At a minimum, each experiment run should record:\n",
    "- Parameters (e.g. learning rate, number of trees, max depth)\n",
    "- Metrics (e.g. accuracy, RMSE, precision, recall)\n",
    "- Data version\n",
    "- Code version (commit ID)\n",
    "- Artifacts (trained model files, plots, feature importance outputs)\n",
    "- Environment information\n",
    "\n",
    "Doing so enables:\n",
    "- Fair comparison between models\n",
    "- Reproducibility of past results\n",
    "- Identification of performance regressions\n",
    "- Collaboration across teams\n",
    "- Selection of a production-ready model\n",
    "\n",
    "Many ML platforms provide built-in functionality for experiment tracking. This usually involves:\n",
    "- A central experiment store\n",
    "- A UI for comparing runs\n",
    "- Automatic logging of metrics and parameters\n",
    "- Integration with model registries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef8c979",
   "metadata": {},
   "source": [
    "### **Model Registry**\n",
    "The model development process typically produces many different models but eventually one is chosen to be the production model. However, over time new data may become available or the performance of the deployed model may deteriorate and so a newer version of the model needs to be trained and deployed. **Model registries** are centralised systems used to:\n",
    "- Store trained models\n",
    "- Version models\n",
    "- Track metadata about each model\n",
    "- Manage promotion to production\n",
    "\n",
    "Model registries make it easy to track which model version was deployed at a given time and rollback to previous versions if needed. Typically, the output of an experiment run is promoted to a registered model version. That version can then be promoted through to production. Inference endpoints can be congifured to either serve the latest version or serve a specific version. This enabled A/B testing, and safe rollback. It also enables canary deployments which is where a second version of a model is deployed on a small percentage of traffic and then this is increased to eventually phase out the older model in a safer way. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
