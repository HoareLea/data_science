{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4031eaf",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "\n",
    "In supervised learning, the goal is to train a model that can make accurate predictions about a specific outcome or phenomenon. The dataset used for training contains both:\n",
    "- Features: measurable characteristics or inputs relevant to the task.\n",
    "- Target values (labels): the correct answers we want the model to learn to predict.\n",
    "\n",
    "Because we already know the correct target values, we can directly evaluate the model’s performance on new, unseen data (the test set) by comparing its predictions against these known answers.\n",
    "\n",
    "Examples of supervised learning could be: \n",
    "- Regression: Predicting the sale value of a home. The dataset would consist of features about homes along with their sales prices \n",
    "- Classification: Predicting whether an image is a cat or not a cat. The dataset would consist of labelled images of cats and not cats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb8ce48",
   "metadata": {},
   "source": [
    "### Unsupervised Learning \n",
    "\n",
    "In unsupervised learning we do not have a measure of what the right answer should be. Rather we are trying to learn underlying patterns that exist within the data. The dataset used contains only features and it is our job to apply the labels. \n",
    "\n",
    "Examples of unsupervised learning could be:\n",
    "- Clustering: Finding groups of similar football players based on their statistics\n",
    "- Dimensionality Reduction: Compressing high-dimensional player performance metrics into 2D or 3D for visualization (e.g. PCA, t-SNE, UMAP).\n",
    "- Anomaly Detection: Identifying unusual players whose stats differ strongly from the rest, which could indicate exceptional talent or data errors.\n",
    "- Association Rule Learning: Discovering rules like “players who excel in passing also tend to have higher possession metrics,” without predefined labels.\n",
    "- Density Estimation: Estimating the distribution of player performance scores to understand typical ranges versus rare outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447013c",
   "metadata": {},
   "source": [
    "### Semi-supervised Learning\n",
    "\n",
    "Semi-supervised learning sits between supervised and unsupervised learning. It leverages both a small amount of labeled data and a much larger pool of unlabeled data. The goal is to improve learning performance by making use of the unlabeled data, which is often easier and cheaper to obtain than labeled examples. This approach is particularly useful when labeling data is expensive, time-consuming, or requires expert knowledge.\n",
    "\n",
    "Label Propagation\n",
    "- Clustering or Graph Construction: Organize the data points into clusters or a similarity graph based on their feature similarities.\n",
    "- Manual Seeding: A human labels a small portion of the clusters (or some representative nodes in the graph).\n",
    "- Propagation: The labels are spread through the structure (e.g., along edges in a graph or within clusters), assigning labels to unlabeled data based on their proximity or similarity to labeled data.\n",
    "- Prediction: Once enough labels are assigned, the resulting labeled dataset can be used to train a standard supervised model.\n",
    "\n",
    "Self-Training (Bootstrapping)\n",
    "- Initial Training: Train a model using the small subset of labeled data.\n",
    "- Pseudo-Labeling: Use the model to predict labels for the unlabeled data.\n",
    "- Confidence Filtering: Select only the predictions the model is most confident about, and add these pseudo-labeled examples to the training set.\n",
    "- Retraining: Re-train the model with the expanded labeled set.\n",
    "- Iteration: Repeat the process until either most of the unlabeled data is labeled or the model performance stabilizes.\n",
    "\n",
    "Label progragation could be used to categorise a large number of news articles. We first cluster them, then manually label and then we can fit unseen artciesl into the labelled clusters.\n",
    "\n",
    "Self-training could be used to categorise a large number of documents into categories based on a small sample of labelled documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f2d64",
   "metadata": {},
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning is a machine learning paradigm where an agent learns to make decisions by interacting with an environment. The agent receives rewards (positive or negative signals) based on its actions, and its goal is to maximize the cumulative reward over time. Unlike supervised learning, where the model learns from labeled input–output pairs, RL relies on trial and error and delayed feedback.\n",
    "\n",
    "For example a robot learning to navigate a maze. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acc4e46",
   "metadata": {},
   "source": [
    "### Ensemble Learning\n",
    "\n",
    "Single models can be prone to overfitting, especiallt if they are highly complex. Ensemble learning tries to overcome this by combining a large number of simple models (called weak learners). The idea is that each model learns about a particular subset of features and that when combined, they produce more robust predictions than single models. \n",
    "\n",
    "Many different machine learning algorithms use this idea although combine the weak learners in slightly different ways:\n",
    "\n",
    "Bagging (Bootstrap Aggregating)\n",
    "- Create multiple datasets by bootstrapping (sampling with replacement from the training set).\n",
    "- Train a simple base learner (e.g., decision tree) on each dataset.\n",
    "- Aggregate their predictions (majority voting for classification, averaging for regression).\n",
    "\n",
    "For example, in random forests, we train many simple decision trees using a different subset of features each time. The predictions from each tree are then combined to get an overall prediciton. \n",
    "\n",
    "Boosting\n",
    "- Train a simple base learner (e.g. decision tree) and use it to make predictions on the training data\n",
    "- Derive the residuals from these predictions\n",
    "- Train another simple base learner but this time use it to make predictions on the residuals\n",
    "- Combine with the previous model \n",
    "- Repeat to create a chain where each model predicts the errors of the previous"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
