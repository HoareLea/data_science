{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e2bc70",
   "metadata": {},
   "source": [
    "### Train, Validation, Test Split\n",
    "\n",
    "Suppose you train a machine learning algorithm on a dataset so that you can make predictions about the underlying process. You have heard that machine learning models usually perform better the more data you train them with and so you train your model using all of your data. Before you deploy your model, you want to check that it will actually perform well. So you test your model using all your data and you find that the model performs really well. Feeling like the greatest data scientst who ever lived, you confidently deploy your model to production. However in production, you notice something strange - your model is performing terribly! Oh no! What has happened?\n",
    "\n",
    "During the training process the model learns patterns which apply well to the data it has 'seen' during the training process. As such, if you test it on this data, it will likely perform very well. However, these patterns may not generalise well to unseen data. To get a more fair indication of how the model might perform in production, you need to test it on 'unseen' data. That is, data not included in the training process. \n",
    "\n",
    "So you randomely split your dataset into 80% for training and 20% for testing. You train the model on the training data and test it on the test data. However the model performs really poorly. Hence you want to make changes to the training process to improve performance. Perhaps processing the data differently, conducting feature engineering, changing the model's hyperparameters etc. You perform these steps and retrain the model on the 80% training dataset. You go to test the model using the remaining 20% but realise there is an issue. Namely, the model has already 'seen' all the test data you have. You already used it to test the model previously so using it again would introduce the same bias you had with the intitial 100% approach. It won't give you a truely unbiased estimate of how the model would perform in production. \n",
    "\n",
    "Now you randomely split your dataset into 70% for training, 20% for validation and 10% for testing. You train the model on the training set and validate it on the validation set. You continue to make improvements to the model, each time validating it on the validation set. Note that repeated validation on the same data would introduce bias and would likely lead to the model being overfit to the validation set. Cross-validation is a technique to address this and is discussed in a separate notebook but for now let's assume this is permissable. After your iterative improvements, you obvserve strong performance on the validation set. Now you are ready to test it one final time using the completely unseen 10% test data. This will give you an unbiased estimate of how your model will perform in production. \n",
    "\n",
    "Note, it is usual to see a small drop in performance on unseen data since the model will not have learned the patterns in this data as well as the patterns in the 'seen' data. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
