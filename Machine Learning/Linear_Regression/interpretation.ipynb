{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f57712",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "One of the key advantages of linear regression is its interpretability. Each coefficient directly represents the size and direction of the relationship between a predictor and the target variable, holding other predictors constant. This makes it straightforward to assess which factors are most influential and whether their effect is positive or negative. In contrast, many machine learning algorithms such as neural networks may achieve higher predictive accuracy, but often function as 'black boxes'. While they can model complex, nonlinear relationships, it is usually much harder to disentangle and explain the contribution of each predictor to the final prediction. Linear regression models, by comparison, provide a transparent, human-readable representation of the underlying process. This makes them particularly valuable not only for prediction but also for developing a deeper understanding of the phenomena under study.\n",
    "\n",
    "In the example below we plot a simple linear model and will discuss each element of the summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87317b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.929\n",
      "Model:                            OLS   Adj. R-squared:                  0.929\n",
      "Method:                 Least Squares   F-statistic:                     6522.\n",
      "Date:                Tue, 02 Sep 2025   Prob (F-statistic):               0.00\n",
      "Time:                        10:14:28   Log-Likelihood:                -1414.8\n",
      "No. Observations:                1000   AIC:                             2836.\n",
      "Df Residuals:                     997   BIC:                             2850.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.9918      0.032     31.432      0.000       0.930       1.054\n",
      "x1             2.0130      0.032     62.921      0.000       1.950       2.076\n",
      "x2             2.9594      0.032     93.802      0.000       2.898       3.021\n",
      "==============================================================================\n",
      "Omnibus:                        2.981   Durbin-Watson:                   2.074\n",
      "Prob(Omnibus):                  0.225   Jarque-Bera (JB):                3.277\n",
      "Skew:                          -0.003   Prob(JB):                        0.194\n",
      "Kurtosis:                       3.280   Cond. No.                         1.04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "n = 1000\n",
    "X = np.random.normal(size=(n, 2))\n",
    "X1 = X[:, 0]\n",
    "X2 = X[:, 1]\n",
    "eps = np.random.normal(size=n)\n",
    "y = 1 + 2 * X1 + 3 * X2 + eps\n",
    "\n",
    "X_with_const = sm.add_constant(X)\n",
    "model = sm.OLS(y, X_with_const)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa72284",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "This section provides the following summary statistics about the model:\n",
    "- Dep. Variable: The name of the response variable (y in this case).\n",
    "- Model: The type of regression used (OLS = ordinary least squares).\n",
    "- Method: How the parameters were estimated (least squares).\n",
    "- No. Observations: Number of data points used (1000).\n",
    "- Df Residuals: Degrees of freedom for residuals = (observations − number of parameters estimated). Here, 1000 − 3 = 997.\n",
    "- Df Model: Number of predictors included (2 in this case: x1 and x2).\n",
    "- R-squared: Proportion of variance in y explained by the predictors. Here 0.929 means the model explains 92.9% of the variation.\n",
    "- Adj. R-squared: Adjusted R² accounts for the number of predictors, penalising unnecessary variables.\n",
    "- F-statistic: Tests whether the model as a whole is statistically significant (very large here = strong evidence that predictors matter).\n",
    "- Prob (F-statistic): The p-value for the F-test (0.00 indicates significance).\n",
    "- Log-Likelihood: A measure of model fit; higher (less negative) is better.\n",
    "- AIC/BIC: Model selection criteria (lower values generally mean better fit, penalising for complexity).\n",
    "- Covariance Type: The type of standard errors reported (here: nonrobust).\n",
    "\n",
    "#### Coefficients\n",
    "This section provides information about the estimates of the coefficients\n",
    "- coef: Model estimate of the coefficient\n",
    "- std err: Standard error in the estimate of the coefficient\n",
    "- t: t-statistic for whether the coefficient is statistically different from 0\n",
    "- P>|t|: p-value for the test of whether the coefficient is statistically different from 0\n",
    "- [0.025: Lower bound of 95% confidence interval for coefficient\n",
    "- 0.975]: Upper bound of 95% confidence interval for coefficient\n",
    "\n",
    "When interpretting the coefficients, it is important to check if they are statistically significant since if not then the model does not think the predictor has any effect on the target. In this case, the predict may be ommitted. It is also useful to check the size of the effect (i.e. the size of the coefficient) since if a predictor only has a very small effect it may also be worth ommitting it. \n",
    "\n",
    "We interpret the coefficients as follows:\n",
    "- \"x1 has a positive impact on y with a unit increase in x1 on average leading to a 2.0130 increase in y, holding all else equal\"\n",
    "- \"x2 has a positive impact on y with a unit increase in x2 on average leading to a 2.9594 increase in y, holding all else equal\"\n",
    "- \"If all predictors are set to 0, on-average we predict y to be 0.9918 (i.e. the constant term)\n",
    "\n",
    "Note that the interpretation of the constant term may not always make logic sense in the real world (e.g. if one of the predictors was height, we can't set height to 0)\n",
    "\n",
    "#### Test Statistics & Diagnostics\n",
    "These statistics help assess whether the assumptions of OLS regression hold and whether the model is reliable:\n",
    "- Omnibus: A combined test for skewness and kurtosis in the residuals. A low value (and high p-value) suggests the residuals are approximately normally distributed.\n",
    "- Prob(Omnibus): The p-value for the Omnibus test. A high value (> 0.05) indicates we fail to reject normality of residuals.\n",
    "- Jarque-Bera (JB): Another test of whether the residuals have skewness and kurtosis consistent with a normal distribution. Like Omnibus, a high p-value suggests normal residuals.\n",
    "- Skew: Measures asymmetry in the distribution of residuals. A value close to 0 indicates symmetry. Positive skew means a long right tail; negative skew means a long left tail.\n",
    "- Kurtosis: Measures the “peakedness” of the residual distribution relative to normal. A value close to 3 indicates normal-like tails. Values > 3 mean heavier tails, < 3 mean - lighter tails.\n",
    "- Durbin-Watson: Tests for autocorrelation (correlation between successive residuals). A value of 2 indicates no autocorrelation. Values < 2 suggest positive autocorrelation, > 2 - suggest negative autocorrelation.\n",
    "- Cond. No. (Condition Number): Indicates multicollinearity (how correlated predictors are). Small values (near 1) mean predictors are independent. Large values (typically > 30) indicate potential multicollinearity problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
