{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31caa44",
   "metadata": {},
   "source": [
    "### k-Fold Cross Validation\n",
    "\n",
    "As discussed in [train_validation_test.ipynb](train_validation_test.ipynb), repeated validation on the same validation set may lead to the model becoming overfit to the validation set. That is, it performs very well on the validation set, but does not generalise well to unseen data. \n",
    "\n",
    "It is important to draw a distinction between this kind of overfitting on the validation set and overfitting on the training set. Overfitting to the training data happens inside the model as the weights/parameters are tuned to fit the noise in the training data. Overfitting to the validation data happens outside the model and is a result of human changes to the machine learning algorithm or its hyperparameters. \n",
    "\n",
    "You repeatedly adjust hyperparameters, features, or preprocessing decisions based on validation performance. Over many iterations, the model selection process itself tunes to the quirks of the validation set. As a result, your chosen model looks great on the validation set (because it has been implicitly optimised for it), but it may not generalise to truly unseen data.\n",
    "\n",
    "In an ideal world, you would have unlimited new data which you could validate the model with each time you change the approach. Cross validation is the next best thing. That is, you resample the same dataset so that each time the validation set is different, although it is drawn from the same pool of data.\n",
    "\n",
    "k-fold cross validation is a common implementation and works as follows:\n",
    "- Combine your training and validation datasets into a single training set\n",
    "- Split this combined training set into k-folds (typically 10 are used)\n",
    "- For each fold, train a model on the k-1 other folds and validate it on the given fold\n",
    "- Average the performance across all k models to estimate model performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e687b0b",
   "metadata": {},
   "source": [
    "### Stratified k-Fold Cross Validation\n",
    "\n",
    "Many classification problems contain highly imbalanced classes. For instance, when trying to detect a disease, most subjects will not have the disease. This can make detecting the minor class very difficult. In a standard k-fold cross validation the folds are generated randomely and as a result of the class imbalance, some folds may contain few few or even zero examples from the minority class. Hence validating the performance of the model on such a fold would lead to misleading performance metrics (undefined recall and misleading precision.)\n",
    "\n",
    "In stratified k-fold cross validation, the propertion of classess is preserved in each fold. So if ~1% of the observations are 'Positive', then ~1% of the examples in each fold should be 'Positive'. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
