{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5b277e",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1b854",
   "metadata": {},
   "source": [
    "### **Hidden State**\n",
    "In a similar way to how CNNs capture the spatial nature of certain data, **Recurrent Neural Networks (RNNs)** model the sequential nature of certain data types such as text and time-series. For such data, the order matters and the past affects the future. It does this via a concept called **hidden state** which represents a compressed sumamry of everything the model has seen so far. Thus it allows the model to incorporate previously seen data. \n",
    "\n",
    "At each timestep $t$ we compute the current hidden state $h_t$ as:\n",
    "\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h h_{t-1})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $W_x$: input weights\n",
    "- $W_h$: reccurent weights\n",
    "- $f$: activation function (usually $tanh(x) = \\frac{e^{2x} - 1}{e^{2x} + 1})$\n",
    "\n",
    "The hidden state is passed forward through time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1110412",
   "metadata": {},
   "source": [
    "### **Long Short-Term Memory (LSTM)**\n",
    "RNNs suffer from several problems:\n",
    "- Gradients can shrink exponentially\n",
    "- Early time steps stop influencing learning\n",
    "- The model struggles with long-term dependencies\n",
    "\n",
    "For example, when trying to predict the next word in the sentence \"I grew up in France, I speak...\", the word 'France' may be too far back for the model to remember. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
